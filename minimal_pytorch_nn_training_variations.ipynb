{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76e0eff2",
   "metadata": {},
   "source": [
    "# Minimal PyTorch NN: Add a Layer + Swap Activation/Optimizer (Teaching Notebook)\n",
    "\n",
    "Goal: **see what changes** when we:\n",
    "- add another hidden layer (more capacity),\n",
    "- change the **activation** (ReLU vs Tanh vs none),\n",
    "- change the **optimizer** (SGD vs Adam).\n",
    "\n",
    "> Note: **SGD is an optimizer**, not an activation. **ReLU is an activation**.  \n",
    "So this notebook shows both kinds of swaps clearly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313f1212",
   "metadata": {},
   "source": [
    "## Local setup (macOS Apple Silicon, e.g. Mac M5) using `uv`\n",
    "\n",
    "### 1) Install `uv`\n",
    "```bash\n",
    "curl -LsSf https://astral.sh/uv/install.sh | sh\n",
    "```\n",
    "\n",
    "### 2) Create a project + virtual env + deps\n",
    "```bash\n",
    "uv init nn-training-demo\n",
    "cd nn-training-demo\n",
    "\n",
    "# Choose a Python version (example: 3.12)\n",
    "uv python install 3.12\n",
    "uv python pin 3.12\n",
    "\n",
    "# Add dependencies\n",
    "uv add torch torchvision torchaudio jupyterlab matplotlib\n",
    "\n",
    "# Create/sync the environment\n",
    "uv sync\n",
    "```\n",
    "\n",
    "### 3) Run Jupyter (or use VS Code with Jupyter extension)\n",
    "```bash\n",
    "uv run jupyter lab\n",
    "```\n",
    "\n",
    "### 4) VS Code kernel tip\n",
    "If VS Code doesn't show your `uv` env as a kernel, install `ipykernel` into the env:\n",
    "```bash\n",
    "uv add ipykernel\n",
    "uv sync\n",
    "```\n",
    "Then restart VS Code and choose the kernel from the top-right **Select Kernel** menu.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4be6c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"torch:\", torch.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa7aeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prefer Apple Metal (MPS) on Apple Silicon if available\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(\"device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d1b9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1) Create a tiny synthetic dataset ---\n",
    "# y = 2x + 3 + noise  (simple on purpose)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "n = 256\n",
    "X = torch.linspace(-2, 2, n).unsqueeze(1)   # (n, 1)\n",
    "noise = 0.3 * torch.randn(n, 1)\n",
    "y = 2.0 * X + 3.0 + noise                   # (n, 1)\n",
    "\n",
    "dataset = TensorDataset(X, y)\n",
    "print(\"X:\", X.shape, \"y:\", y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162f3960",
   "metadata": {},
   "source": [
    "## 2) Define model variants\n",
    "\n",
    "We'll compare these model choices:\n",
    "\n",
    "**A) Baseline (1 hidden layer + ReLU)**\n",
    "- Linear(1→8) → ReLU → Linear(8→1)\n",
    "\n",
    "**B) Deeper (2 hidden layers + ReLU)**\n",
    "- Linear(1→8) → ReLU → Linear(8→8) → ReLU → Linear(8→1)\n",
    "\n",
    "**C) Activation swap**\n",
    "- Replace ReLU with Tanh (a \"squashing\" activation)\n",
    "\n",
    "**D) No activation**\n",
    "- Linear(1→8) → Linear(8→1)  \n",
    "  (This is effectively *still linear overall*; good to demonstrate why activations matter.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d5d29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(kind: str) -> nn.Module:\n",
    "    if kind == \"baseline_relu\":\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(1, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, 1),\n",
    "        )\n",
    "    if kind == \"deeper_relu\":\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(1, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, 1),\n",
    "        )\n",
    "    if kind == \"baseline_tanh\":\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(1, 8),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(8, 1),\n",
    "        )\n",
    "    if kind == \"no_activation\":\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(1, 8),\n",
    "            nn.Linear(8, 1),\n",
    "        )\n",
    "    raise ValueError(f\"Unknown kind: {kind}\")\n",
    "\n",
    "def count_params(model: nn.Module) -> int:\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7906e1d8",
   "metadata": {},
   "source": [
    "## 3) One training function (shows the 5-step loop)\n",
    "\n",
    "Inside each batch:\n",
    "1) forward pass  \n",
    "2) compute loss  \n",
    "3) `zero_grad()`  \n",
    "4) `backward()`  \n",
    "5) `step()`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61f4653",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model: nn.Module, optimizer_name: str, lr: float, epochs: int = 50, batch_size: int = 32, seed: int = 0):\n",
    "    # Reset seeds for fair-ish comparisons\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    # Fresh loader with deterministic shuffle (as much as possible)\n",
    "    g = torch.Generator()\n",
    "    g.manual_seed(seed)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, generator=g)\n",
    "\n",
    "    model = model.to(device)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    if optimizer_name.lower() == \"sgd\":\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    elif optimizer_name.lower() == \"adam\":\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    else:\n",
    "        raise ValueError(\"optimizer_name must be 'sgd' or 'adam'\")\n",
    "\n",
    "    history = []\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        running = 0.0\n",
    "\n",
    "        for xb, yb in loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "\n",
    "            # (1) forward\n",
    "            y_pred = model(xb)\n",
    "\n",
    "            # (2) loss\n",
    "            loss = loss_fn(y_pred, yb)\n",
    "\n",
    "            # (3) clear grads\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # (4) backprop\n",
    "            loss.backward()\n",
    "\n",
    "            # (5) update\n",
    "            optimizer.step()\n",
    "\n",
    "            running += loss.item() * xb.size(0)\n",
    "\n",
    "        epoch_loss = running / len(dataset)\n",
    "        history.append(epoch_loss)\n",
    "\n",
    "    # Predictions for visualization\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_hat = model(X.to(device)).cpu()\n",
    "\n",
    "    return history, y_hat, model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f559c40a",
   "metadata": {},
   "source": [
    "## 4) Run experiments\n",
    "\n",
    "We'll keep the problem, data, and epochs the same, and compare:\n",
    "\n",
    "### Part 1 — Add a layer (capacity increase)\n",
    "- baseline_relu + SGD\n",
    "- deeper_relu + SGD\n",
    "\n",
    "### Part 2 — Swap activation\n",
    "- baseline_relu + SGD\n",
    "- baseline_tanh + SGD\n",
    "- no_activation + SGD\n",
    "\n",
    "### Part 3 — Swap optimizer (SGD vs Adam)\n",
    "- baseline_relu + SGD\n",
    "- baseline_relu + Adam\n",
    "\n",
    "> Tip: hyperparameters matter. A deeper model sometimes benefits from a slightly smaller learning rate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a72596",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = [\n",
    "    # Part 1: add a layer\n",
    "    {\"name\": \"baseline_relu + SGD(lr=0.10)\", \"kind\": \"baseline_relu\", \"opt\": \"sgd\", \"lr\": 0.10},\n",
    "    {\"name\": \"deeper_relu + SGD(lr=0.05)\",  \"kind\": \"deeper_relu\",  \"opt\": \"sgd\", \"lr\": 0.05},\n",
    "\n",
    "    # Part 2: activation swap\n",
    "    {\"name\": \"baseline_tanh + SGD(lr=0.10)\", \"kind\": \"baseline_tanh\", \"opt\": \"sgd\", \"lr\": 0.10},\n",
    "    {\"name\": \"no_activation + SGD(lr=0.10)\", \"kind\": \"no_activation\", \"opt\": \"sgd\", \"lr\": 0.10},\n",
    "\n",
    "    # Part 3: optimizer swap\n",
    "    {\"name\": \"baseline_relu + Adam(lr=0.05)\", \"kind\": \"baseline_relu\", \"opt\": \"adam\", \"lr\": 0.05},\n",
    "]\n",
    "\n",
    "results = {}\n",
    "\n",
    "for cfg in experiments:\n",
    "    torch.manual_seed(0)  # make init comparable\n",
    "    model = make_model(cfg[\"kind\"])\n",
    "    params = count_params(model)\n",
    "\n",
    "    hist, y_hat, trained = train_model(model, cfg[\"opt\"], cfg[\"lr\"], epochs=50, batch_size=32, seed=0)\n",
    "    results[cfg[\"name\"]] = {\"history\": hist, \"y_hat\": y_hat, \"params\": params}\n",
    "\n",
    "    print(f\"{cfg['name']:<30} | params={params:<4} | final_loss={hist[-1]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5457fe8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Compare loss curves ---\n",
    "plt.figure()\n",
    "for name, r in results.items():\n",
    "    plt.plot(r[\"history\"], label=name)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"MSE loss\")\n",
    "plt.title(\"Loss curves (lower is better)\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2052f291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Compare predictions ---\n",
    "# We'll plot the true data (scatter) and each model's prediction (line).\n",
    "plt.figure()\n",
    "plt.scatter(X, y, s=10, label=\"data\")\n",
    "\n",
    "for name, r in results.items():\n",
    "    plt.plot(X, r[\"y_hat\"], linewidth=2, label=name)\n",
    "\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"Predictions: baseline vs deeper vs activation/optimizer swaps\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91907cbb",
   "metadata": {},
   "source": [
    "## 5) Teaching takeaways (what to point out live)\n",
    "\n",
    "**Adding a hidden layer (more depth/capacity):**\n",
    "- Can fit more complex patterns *in general*.\n",
    "- On this simple linear-ish dataset, you may not need extra capacity.\n",
    "- Deeper models sometimes need different learning rates to train smoothly.\n",
    "\n",
    "**Changing activation:**\n",
    "- ReLU often trains quickly and avoids saturating gradients.\n",
    "- Tanh can be fine but may train differently because it “squashes” values.\n",
    "- No activation makes the network effectively linear overall → it can’t represent non-linear relationships.\n",
    "\n",
    "**Changing optimizer:**\n",
    "- SGD is simple and transparent; great for teaching.\n",
    "- Adam often converges faster/easier with less tuning (but is less “bare metal”).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
